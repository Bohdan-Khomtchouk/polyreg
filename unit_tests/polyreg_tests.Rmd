---
title: "Unit Tests for polyreg"
date: "9/22/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, comment="")
```

This document tests out key features of `polyreg`. We will start with `pe` data, which is found in `library(regtools)`.

```{r}
library(polyreg)
regtools::getPE(Dummies=TRUE) 
```

## predicting wage

```{r}
pe1 <- pe[,c(1,2,4,12:16,3)]
set.seed(9999)
idxs <- sample(1:nrow(pe1),2500,replace=FALSE)
pe1trn <- pe1[-idxs,]
pe1tst <- pe1[idxs,]

pfout <- polyFit(pe1trn,2)
ypred <- predict(pfout,pe1tst[,-9]); mean(abs(ypred-pe1tst[,9]))  
# 25962.43
```

# xvalPoly()
```{r}
set.seed(9999)
xvalPoly(pe1,2) 
# 25764.98 25141.50
```
# Different PCA methods
```{r}
pfout <- polyFit(pe1trn,2,pcaMethod='prcomp')
ypred <- predict(pfout,pe1tst[,-9]); mean(abs(ypred-pe1tst[,9]))  
# 27217.16
```
```{r}
pfout <- polyFit(pe1trn,2,pcaMethod='RSpectra',pcaPortion=2)
ypred <- predict(pfout,pe1tst[,-9]); mean(abs(ypred-pe1tst[,9])) 
# 27217.16
```
```{r}
pfout <- polyFit(pe1trn,2,pcaMethod='prcomp',pcaLocation='back')
ypred <- predict(pfout,pe1tst[,-9]); mean(abs(ypred-pe1tst[,9])) 
# 27388.96
```

```{r}
pfout <-
polyFit(pe1trn,2,pcaMethod='RSpectra',pcaPortion=3,pcaLocation='back')
ypred <- predict(pfout,pe1tst[,-9]); mean(abs(ypred-pe1tst[,9]))  
# 27388.96
```
# predicting occ
```{r}
pe2 <- pe1
pe2 <- pe2[,c(1:3,9,4:8)]
pe2$occ6 <- 1 - apply(pe2[,5:9],1,sum)
pe2$occ <- apply(pe2[,5:10],1,which.max)
pe2[,5:10] <- NULL
```

```{r}
set.seed(9999)
idxs <- sample(1:nrow(pe1),2500,replace=FALSE)
pe2trn <- pe2[-idxs,]
pe2tst <- pe2[idxs,]
```

```{r, echo=FALSE}
# in next example, strange RMarkdown error; results pasted from Console
```

```{r, eval=FALSE}
pfout <- polyFit(pe2trn,2,use='glm')
```
```
getPoly time:  0.031 0.005 0.035 0 0 
one-vs-all glm() time:  0.504 0.17 0.711 0.014 0.005 
```
```{r, eval=FALSE}
ypred <- predict(pfout,pe2tst[,-5]); mean(ypred==pe2tst[,5]) 
# 0.3936

pfout <- polyFit(pe2trn,2,use='mvrlm')
```
```
getPoly time:  0.03 0.005 0.035 0 0 
one-vs-all glm() time:  0.493 0.162 0.69 0.013 0.005 
```
```{r, eval=FALSE}
ypred <- predict(pfout,pe2tst[,-5]); mean(ypred==pe2tst[,5])  
# 0.3964
```

# FSR

Here are some tests for `FSR()`, the forward stepwise regression function. Under the hood, the `block_solve()` algorithm eases memory use and should yield the same results as `solve()` (up to rounding). 
```{r block_solve}
X <- as.matrix(mtcars)
baseR_approach  <- solve(crossprod(X)) 
baseR_approach[1:5, 1:5]
polyreg_approach1 <- polyreg:::block_solve(X=X)
polyreg_approach1[1:5, 1:5]
max(abs(baseR_approach - polyreg_approach1)) < 10^{-11}
polyreg_approach2 <- polyreg:::block_solve(S=crossprod(X))
polyreg_approach2[1:5, 1:5]
max(abs(baseR_approach - polyreg_approach2)) < 10^{-11}
```
When `FSR()` estimates coefficients via Ordinary Least Squares, it should yield the same results as `lm()`.
```{r ols}
baseR_beta <- coef(lm(carb ~ ., mtcars))
baseR_beta 

X <- cbind(1, as.matrix(mtcars[,-ncol(mtcars)]))
y <- mtcars$carb
XtX_inv <- polyreg:::block_solve(X = X, max_block = 250)
polyreg_beta <- tcrossprod(XtX_inv, X) %*% y
polyreg_beta

max(abs(baseR_beta - polyreg_beta)) < 10^{-9}
```
`FSR()` works for different input types and uses different types of estimation. Specifically, continuous, binary, and multinomial outcomes may be estimated using OLS (as implemented above). Binary outcomes may also be estimated via `glm` and multinomial via `multinom::nnet`, respectively. Here is a simple way to make sure all five scenarios are running...

```{r}
l <- FSR(mtcars, seed = 9999, noisy = FALSE)
summary(l)

B <- cbind(mtcars, as.factor(sample(letters[1:2], nrow(mtcars), replace=TRUE)))
colnames(B)[ncol(B)] <- "idk"

b1 <- FSR(B, seed = 9999, noisy=FALSE)
summary(b1)
b2 <- FSR(B, seed = 9999, linear_estimation = TRUE, noisy=FALSE)
summary(b2)

c1 <- FSR(iris, seed = 9999, noisy=FALSE)
summary(c1)
c2 <- FSR(iris, seed = 9999, linear_estimation = TRUE, noisy=FALSE)
summary(c2)

```


